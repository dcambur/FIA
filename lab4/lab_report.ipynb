{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d252a8d",
   "metadata": {
    "id": "7d252a8d"
   },
   "source": [
    "# FAF.FIA16.1 -- Artificial Intelligence Fundamentals\n",
    "\n",
    "> **Lab 4:** Processing Images with OpenCV \\\\\n",
    "> **Performed by:** Cambur Dumitru, group FAF-191 \\\\\n",
    "> **Verified by:** Mihail Gavrilita\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BPiGwyyGNsHh",
   "metadata": {
    "id": "BPiGwyyGNsHh"
   },
   "source": [
    "## Imports and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fe5324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-23T09:47:05.838671Z",
     "start_time": "2022-01-23T09:47:05.834860Z"
    },
    "id": "533fd9fa"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0493d",
   "metadata": {},
   "source": [
    "## Task 1 -- Write the following functions using OpenCV. Adjust the parameters and explain your approach. Plot the initial image and the blurred image in the same plot by using Matplotlib subplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d6309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 9] [-1, -1] [6, 12] [4.0, 8.0] 26 -2\n"
     ]
    }
   ],
   "source": [
    "def blur_image(img, kernel_size=5):\n",
    "    blurred = cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "    return blurred\n",
    "\n",
    "\n",
    "def sharpen_image(img, kernel_size=5, amount=1, threshold=0):\n",
    "    blurred = cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "    sharpened = cv2.addWeighted(img, 1 + amount, blurred, -amount, threshold)\n",
    "    return sharpened\n",
    "\n",
    "\n",
    "img = cv2.imread('test_images/0AA0A2.jpg')\n",
    "blurred_img = blur_image(img)\n",
    "sharpened_img = sharpen_image(img)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].imshow(cv2.cvtColor(blurred_img, cv2.COLOR_BGR2RGB))\n",
    "ax[1].set_title('Blurred Image')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "ax[0].set_title('Original Image')\n",
    "ax[1].imshow(cv2.cvtColor(sharpened_img, cv2.COLOR_BGR2RGB))\n",
    "ax[1].set_title('Sharpened Image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0f6af",
   "metadata": {},
   "source": [
    "In the code above, for the sharpening algorithm \"unsharp masking\" is applied. Its name derives from the fact that the technique uses a blurred, or \"unsharp\", negative image to create a mask of the original image. The unsharp mask is then combined with the original positive image, creating an image that is less blurry than the original. The resulting image, although clearer, may be a less accurate representation of the image's subject. To control the unsharp masking typically amount, radius and threshold is used:\n",
    "\n",
    "    Amount is listed as a percentage and controls the magnitude of each overshoot (how much darker and how much lighter the edge borders become). This can also be thought of as how much contrast is added at the edges. It does not affect the width of the edge rims.\n",
    "    \n",
    "    Radius affects the size of the edges to be enhanced or how wide the edge rims become, so a smaller radius enhances smaller-scale detail. Higher radius values can cause halos at the edges, a detectable faint light rim around objects. Fine detail needs a smaller radius. Radius and amount interact; reducing one allows more of the other.\n",
    "    \n",
    "    Threshold controls the minimal brightness change that will be sharpened or how far apart adjacent tonal values have to be before the filter does anything. This lack of action is important to prevent smooth areas from becoming speckled. The threshold setting can be used to sharpen more pronounced edges, while leaving subtler edges untouched. Low values should sharpen more because fewer areas are excluded. Higher threshold values exclude areas of lower contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938e3e4",
   "metadata": {
    "id": "0938e3e4"
   },
   "source": [
    "## Task 2 -- Implement a face detection system using OpenCV. The function should take as input one image and output the result as the coordinates of the face, in case the image contains a face, or None if the image does not contain any faces. Assume that the image contains no more than one face.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0859a4",
   "metadata": {
    "id": "6b0859a4"
   },
   "outputs": [],
   "source": [
    "def detect_face(image):\n",
    "    # Load the pre-trained face detection classifier\n",
    "    face_cascade = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    # Check if a face is detected\n",
    "    if len(faces) > 0:\n",
    "        # Assume that the image contains only one face\n",
    "        # Return the coordinates of the detected face\n",
    "        (x, y, w, h) = faces[0]\n",
    "        #cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        #cv2.imshow(\"img\", img)\n",
    "        #cv2.waitKey()\n",
    "        return x, y, x + w, y + h\n",
    "    else:\n",
    "        # If no faces are detected, return None\n",
    "        return None\n",
    "\n",
    "\n",
    "img = cv2.imread('test_images/0AA0A2.jpg')\n",
    "face = detect_face(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05df7a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f67a3d5",
   "metadata": {
    "id": "3f67a3d5"
   },
   "source": [
    "## Task 3 -- Task 3 Implement a system that detects if a photo is accepted for passport or not, by using OpenCV. You can be creative in determining the optimal strategy, but the system should at least follow the listed requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d0bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_passport_photo(image):\n",
    "    # Check if the image is colored\n",
    "    if len(image.shape) < 3 or image.shape[2] < 3:\n",
    "        return False\n",
    "\n",
    "    # Check if the image is in portrait or square orientation\n",
    "    (height, width, _) = image.shape\n",
    "    if height > width:\n",
    "        image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "        (height, width, _) = image.shape\n",
    "    if height != width:\n",
    "        return False\n",
    "\n",
    "    # Detect the eyes in the image\n",
    "    eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    eyes = eye_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    if len(eyes) != 2:\n",
    "        return False\n",
    "\n",
    "    # Check if the eyes are at the same level\n",
    "    (eye1_x, eye1_y, eye1_w, eye1_h) = eyes[0]\n",
    "    (eye2_x, eye2_y, eye2_w, eye2_h) = eyes[1]\n",
    "    eye1_center = eye1_y + (eye1_h / 2)\n",
    "    eye2_center = eye2_y + (eye2_h / 2)\n",
    "    if abs(eye1_center - eye2_center) > 5:\n",
    "        return False\n",
    "\n",
    "    # Check if the photo contains only one person\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    if len(faces) != 1:\n",
    "        return False\n",
    "\n",
    "    # Check if the head of the person represents 20% to 50% of the area of the photo\n",
    "    (x, y, w, h) = faces[0]\n",
    "    face_area = w * h\n",
    "    image_area = width * height\n",
    "    if face_area < (0.2 * image_area) or face_area > (0.5 * image_area):\n",
    "        return False\n",
    "\n",
    "    # If all checks pass, the photo is accepted\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50fc00",
   "metadata": {},
   "source": [
    "The main algorithm used to detect different features in an image is haar cascade. Haar cascade uses the cascading window, and it tries to compute features in every window and classify whether it could be an object. One can divide the algorithm into 4 steps:\n",
    "\n",
    "1)Haar Features calculation (represented by taking a rectangular part of an image and dividing that rectangle into multiple parts)\n",
    "\n",
    "2)Creation of Integral Images (image is where each pixel represents the cumulative sum of a corresponding input pixel with all pixels above and left of the input pixel. Speeds up the first step)\n",
    "\n",
    "3)Using Adaboost (essentially chooses the best features and trains the classifiers to use them. It uses a combination of “weak classifiers” to create a “strong classifier” that the algorithm can use to detect objects)\n",
    "\n",
    "4)Implementing Cascading Classifier itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79f8f1",
   "metadata": {
    "id": "046623ad"
   },
   "source": [
    "## Task 4 --  Test how good your system performs on a test dataset. You are required to apply your system to all the images in the test set, then compute the accuracy for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f96009c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from task3 import is_passport_photo\n",
    "from task1 import blur_image, sharpen_image\n",
    "test_folder = \"test_images/\"\n",
    "test_csv_file = \"test.csv\"\n",
    "\n",
    "# Read the labels from the test.csv file\n",
    "with open(test_csv_file, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    labels = {row['new_path']: row['label'] == 'True' for row in reader}\n",
    "\n",
    "# Test the passport photo validator on each image in the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "for image_path, label in labels.items():\n",
    "    image = cv2.imread(image_path)\n",
    "    blur = blur_image(image)\n",
    "    sharp = sharpen_image(blur)\n",
    "\n",
    "    if is_passport_photo(sharp) == label:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3978b",
   "metadata": {},
   "source": [
    "## Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cclcyPqeSFL",
   "metadata": {
    "id": "3cclcyPqeSFL"
   },
   "source": [
    "After implementing this laboratory work I've learnt about various image processing algorithms like blurring (smoothening), sharpening, haarcascade model innerworks and implemented all of this into a single project which identifies if image follows a required set of requirements to be applied into a passport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rTJNie6deHsg",
   "metadata": {
    "id": "rTJNie6deHsg"
   },
   "source": [
    "## Bibliography:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gUxrDWvseZie",
   "metadata": {
    "id": "gUxrDWvseZie"
   },
   "source": [
    "https://en.wikipedia.org/wiki/Unsharp_masking\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2022/04/object-detection-using-haar-cascade-opencv/\n",
    "\n",
    "https://medium.com/analytics-vidhya/haar-cascades-explained-38210e57970d"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
